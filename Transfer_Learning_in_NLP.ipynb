{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a90d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment setup for Transformers + TF/PyTorch interop ---\n",
    "# You may need to restart the kernel after this cell finishes installing packages.\n",
    "import sys\n",
    "\n",
    "def _pip(cmd):\n",
    "    import subprocess, sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\"] + cmd.split())\n",
    "\n",
    "try:\n",
    "    import transformers, tensorflow, safetensors  # noqa: F401\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "# Install/upgrade compatible versions\n",
    "_pip('install -U \"transformers>=4.44\" \"tensorflow>=2.15,<2.17\" \"safetensors>=0.4.2\" \"torch>=2.2\"')\n",
    "print(\"âœ… Environment prepared. If you see import errors later, restart the kernel and rerun.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK39__EXXm_d"
   },
   "source": [
    "# **Transfer Learning**\n",
    "\n",
    "Transfer learning is a machine learning technique in which knowledge gained through one task or dataset is used to improve model performance on another related task and/or different dataset.\n",
    "\n",
    "### Why Transfer Learning is important in NLP?\n",
    "\n",
    "Transfer Learning plays a vital role in Natural Language Processing (NLP) because it allows knowledge gained from one task or domain to be used in another, usually related, task or domain. This method is particularly beneficial in NLP for several reasons:\n",
    "\n",
    "1. **Data Efficiency**: NLP models generally need a large amount of labeled data to perform effectively. Transfer Learning helps by pretraining models on extensive datasets, like Wikipedia, and then fine-tuning them on smaller, task-specific datasets. This reduces the necessity for large amounts of labeled data for each specific task.\n",
    "\n",
    "2. **Resource Savings**: Training large NLP models from scratch is often costly and time-consuming. Using a pretrained model for fine-tuning requires fewer resources, making it more practical for researchers and practitioners.\n",
    "\n",
    "3. **Performance Improvement**: Pretrained models already contain useful linguistic features and patterns learned from vast text data. Fine-tuning these models for specific tasks usually enhances performance, especially when there is limited labeled data available.\n",
    "\n",
    "4. **Domain Adaptation**: Transfer Learning allows models to adapt to new domains or languages with minimal additional training, making it essential for NLP applications that need to perform well across various domains and languages.\n",
    "\n",
    "5. **Continual Learning**: A model trained through Transfer Learning can be easily updated or adjusted with new data, enabling it to continuously learn and improve its performance over time.\n",
    "\n",
    "### How Transfer Learning in NLP Works\n",
    "\n",
    "1.   **Pre-training on Large Datasets**: Initially, models are trained on extensive and diverse text corpora to learn general language features, such as syntax and semantics. This is done using methods like masked language modeling or autoregressive language modeling.\n",
    "\n",
    "2.   **Fine-Tuning on Specific Tasks**: After pre-training, these models are fine-tuned with smaller, specialized datasets to adjust their parameters for specific tasks, such as sentiment analysis or question answering.\n",
    "\n",
    "3.   **Efficiency and Performance**: Transfer learning greatly reduces the need for extensive computational resources and time for training. It also enhances model performance, particularly in situations where there is limited data.\n",
    "\n",
    "4.   **Applications Across Domains**: This approach is effective for adapting models to specialized domains, such as legal or medical fields, and for transferring knowledge from models trained in one language to others.\n",
    "\n",
    "5.   **Challenges**: There can be challenges, such as mismatches between the data used in pre-training and the specific task data, as well as the high computational demands associated with using large, complex models.\n",
    "\n",
    "### List of Transfer Learning NLP Models\n",
    "\n",
    "Here's a list of notable models in natural language processing that utilize transfer learning, each recognized for their unique contributions and advancements:\n",
    "\n",
    "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT uses a transformer-based architecture. It improves model understanding by employing techniques like masked language modeling and next sentence prediction.\n",
    "\n",
    "2. **GPT (Generative Pre-trained Transformer)**: Created by OpenAI, GPT models are known for their strength in text generation, using autoregressive language modeling during their training.\n",
    "\n",
    "3. **T5 (Text-To-Text Transfer Transformer)**: An innovation from Google, T5 reformulates all natural language processing tasks into a text-to-text framework, treating both inputs and outputs as text strings.\n",
    "\n",
    "4. **DistilBERT**: This streamlined version of BERT is designed to be smaller and faster while maintaining most of BERTâ€™s original language understanding capabilities.\n",
    "\n",
    "5. **BART (Bidirectional and Auto-Regressive Transformers)**: BART combines the bidirectional training of BERT and the autoregressive features of GPT. It is trained by corrupting texts and learning to accurately reconstruct the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXyTHgykZTqN"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "Let's have a quick look at the ðŸ˜‰ Transformers library features. The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.\n",
    "\n",
    "First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. Then, we will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6PEQ71fZZ1N"
   },
   "source": [
    "# Getting started on a task with a pipeline\n",
    "\n",
    "The easiest way to use a pretrained model on a given task is to use pipeline. ðŸ˜‰ Transformers provides the following tasks out of the box:\n",
    "\n",
    "* Sentiment analysis: is a text positive or negative?\n",
    "* Text generation (in English): provide a prompt and the model will generate what follows.\n",
    "* Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
    "* Question answering: provide the model with some context and a question, extract the answer from the context.\n",
    "* Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\n",
    "* Summarization: generate a summary of a long text.\n",
    "* Translation: translate a text in another language.\n",
    "* Feature extraction: return a tensor representation of the text.\n",
    "\n",
    "Let's see how this work for sentiment analysis (the other tasks are all covered in the task summary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipL7kgjYZ56h",
    "outputId": "94d548f8-ccf6-49f4-d2cf-e9481c0d31a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "22122b1b35ce4e8fbe93569c32641d7f",
      "d57a976a27e34ba6ab5c759d6b845725",
      "f5ae91c456ce4abca81d1500f1d98799",
      "6331ffb40e9b4ce4a24ba64bc3aacfcb",
      "a8e3086f549e43a48005f1c1751a980a",
      "f8b45b825913450ba03c50b55400eab5",
      "b213a9f447d241738899af9400846d29",
      "5d16ad3bfb0a42868e7799d083aeb04f",
      "1bbc3f9678244afe88e388882e8d5991",
      "5dcc8cc7a2f34e9aab3570e1565d30d2",
      "414922e3a5ca49a087227737455ea507",
      "c23b54bba31047d09406bb7a6bb414e2",
      "0b24fd518d884eefae0dc2302ec13851",
      "df97bb4fef7a452ba5c16e8518c46796",
      "d506138551da4f03be02a9bcae5f9c4e",
      "8d692faa5e834da28d22b552fdfbd36d",
      "487cf343e3484ddfaede1f157cef3794",
      "ff191e726bc14664b853b1e16ce5ca82",
      "4cd7687976674663ab1ab997426988ac",
      "04e6ffd1620e46c18631ccfab6360dac",
      "7ba395980ec04a3992ef58b4461264d2",
      "bd6eb00eea4e47678f7bd350e689c6f0",
      "8772b223ff324da1af3066d604718484",
      "733199d628044757926db7e72e135698",
      "6f12a15fe2304cb19e2f1e6929d9b0e7",
      "d7ca35c92b514c20af6d0bf6d7a7c0cd",
      "abc48e2ed6034729a3a7bb9da2346a8b",
      "ad4fe387610846acb2dc99202229d8c8",
      "ae2a89540bf74b2c9f81ad3862db61f3",
      "6415226722c34ed892bd6104dc7e1de0",
      "4e307cca2bc34d7896bee5ff3fcf2083",
      "3baf80713a9441538f96421283069f84",
      "dca98fb7e46a4cd4a1a8da26a7a1be39",
      "04589292bc7945548273afbe9f34013e",
      "fcdf50cbddd346ffa4c7238e94f83445",
      "321870ed23b940a99660695e27b224e4",
      "b1f98e00f632464fb2058cc15ae9ebb9",
      "0bbd63acb27d418cb41526fdc62793ab",
      "51c0fe36012f4dbf9cc67070303f3783",
      "56ed958bf3ea47a098ea39eb742c70ac",
      "aa62605da98340efbd75b4a4848a2eca",
      "d7416c95e0624f22a6bab042d7189a11",
      "8da7a6bf33784286bcf240aa14e78557",
      "49794ca7a02d468a9bfabf8f60f4537c"
     ]
    },
    "id": "fwAjhgnjZ7RX",
    "outputId": "1a29db79-2cbe-439c-e06f-b5620c98809f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22122b1b35ce4e8fbe93569c32641d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23b54bba31047d09406bb7a6bb414e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8772b223ff324da1af3066d604718484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04589292bc7945548273afbe9f34013e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kl8G3IS8Z80w",
    "outputId": "eb27c5d6-ad28-4d08-b186-80300eb6fe8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998376369476318}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are very happy to be a part of this LLM course')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ep1wsPxCaqKb",
    "outputId": "fc64350b-6180-4279-cda9-496b05325154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9933103322982788}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('The LLM course is great except this module is too long')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swNA3JjNasbT",
    "outputId": "8076e968-08d7-4df1-c0e6-ded8b4da9e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ˜‰ Transformers library.\",\n",
    "                      \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYClYiDma3Zh"
   },
   "source": [
    "You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is fairly neutral.\n",
    "\n",
    "By default, the model downloaded for this pipeline is called \"distilbert-base-uncased-finetuned-sst-2-english\". We can look at its [model page](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
    ")  to get more information about it. It uses the [DistilBERT architecture](https://huggingface.co/transformers/model_doc/distilbert.html\n",
    ") and has been fine-tuned on a dataset called SST-2 for the sentiment analysis task.\n",
    "\n",
    "Let's say we want to use another model; for instance, one that has been trained on French data. We can search through the [model hub](https://huggingface.co/models\n",
    ") that gathers models pretrained on a lot of data by research labs, but also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags \"French\" and \"text-classification\" gives back a suggestion \"nlptown/bert-base-multilingual-uncased-sentiment\". Let's see how we can use it.\n",
    "\n",
    "You can directly pass the name of the model to use to pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "ccd4ef90ab9b4124802ac0583514e6e4",
      "71f3a24180a84448b1aee153b8ebf031",
      "5af0e754592b4ce9ad6305193b53ab10",
      "8a29069f7d8d47a7b04b66960e925218",
      "857f829d4a71461199c4e192c0993288",
      "c302877be2e74426a0a47d0475530f0a",
      "7e59e5ce7ba24c6eb02fc3fc1876af87",
      "d503082891454b8c9b28205ef0c3600e",
      "3fbe4c1752664cf5815a6e087d258f1d",
      "78061c4d2ea14998a86d09e145a36216",
      "b8f066773a5f47cfb3bc12ea4becce57",
      "31df25e4bab14cfb87b079874c0ae3a5",
      "6eb0b2878c544c588f02c62935663359",
      "a26c43d0799544469e1c58ae3ed6b08d",
      "fa470672076544fc9b65fbed247455c3",
      "0c05b0578e8247b4b059b8b98f2ff027",
      "538dc7ad33a84f8d816f4c9390418691",
      "f570f41e69b0431bb09e9d7a612839be",
      "93b007c9ab8e434eb810cc54ca804602",
      "87c101720e40431185b89d022acfb289",
      "1cc006663bb74f9dbc4744ee120405c1",
      "d50c94c4b4ef4e9c925ca0d501dd5874",
      "0ccb1fb5729d4c1b8f210bb49b11dc00",
      "40e48f018ee847ec944f0662f59754fc",
      "97d687cb05bc4ec89f80b499d6e42682",
      "cda92b73d211424e87ede39e78949e87",
      "6bb1d28021f74ed08b56f4e60cf7b4e7",
      "f6be16a9d7724333b224beceee5e430a",
      "ac1756c1d91549338fec239a5ae0d9a2",
      "21e74675e6ac4264b6b9a04ada93ac11",
      "0d14e4bd27bb4d939365140ece5b3a80",
      "794e5fe4a8444a04b88291ea6b3b4d4a",
      "02a52b98b7084ba7aa7660633b92d9bd",
      "e814a30e16994b6da0ff68f191b91079",
      "846024e024494e9b9b5535eb28bfc78b",
      "88c5850adbb84cc38c759441af2f0938",
      "0e16ddfb3c964caca106a2c6482f29c7",
      "4bd6acf3e97d4a149fef567431a307ae",
      "bbbdf51042804c2ead311d83bd7e7e7b",
      "6807e20fbc5d4c78912c6bc6260b0e16",
      "19d9b4060fc04a60ad4857f278836d58",
      "b89ee55fa01c4b63ad71f0ee4132bfb5",
      "0843f1480c0e41a0b5c286cbcc6eae1e",
      "1e65d26503974c6b9e71d023a4969532",
      "f87ce566b49448d2b538003bf835cdad",
      "f0b1b7b8996044f5992ee4f766ed9f97",
      "d80d2ddcc76644dcaaa1d94d9980d7ed",
      "851b95684dd7471fb61cd857041f8ca5",
      "e1402f11ab604a96953a9a3355dae0fb",
      "b7112a008aa346bf8bb3570b10036e5e",
      "601ee66298c6462e973eb71c3e2b2177",
      "b377fa2b8b0b4418af904f8a5b232be3",
      "35b1085788aa44fa9eb1d4caf08f1391",
      "783b1b8e25c24e4197474f8b490dd888",
      "6539e0c1581d43929fe7622f33f62236"
     ]
    },
    "id": "v9GnyMDQcOyi",
    "outputId": "fa41c875-f99a-4e9f-bd3a-a3c456427804"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd4ef90ab9b4124802ac0583514e6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31df25e4bab14cfb87b079874c0ae3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb1fb5729d4c1b8f210bb49b11dc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e814a30e16994b6da0ff68f191b91079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87ce566b49448d2b538003bf835cdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJPGVI0rcPyx",
    "outputId": "12e5e804-a6df-4d7e-9260-c12813446920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '3 stars', 'score': 0.33688199520111084}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Esperamos que no lo odie.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxLN0GA-cqmM"
   },
   "source": [
    "This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model object and its associated tokenizer.\n",
    "\n",
    "We will need two classes for this. The first is AutoTokenizer, which we will use to download the tokenizer associated to the model we picked and instantiate it. The second is AutoModelForSequenceClassification (or TFAutoModelForSequenceClassification if you are using TensorFlow), which we will use to download the model itself. Note that if we were using the library on an other task, the class of the model would change. The task summary tutorial summarizes which class is used for which task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff5CAzVeddvQ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp3nUuPsdh6L"
   },
   "source": [
    "Now, to download the models and tokenizer we found previously, we just have to use the\n",
    "AutoModelForSequenceClassification.from_pretrained method (feel free to replace model_name by any other model from the model hub):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKBAA1BXdqUK"
   },
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vikhAwdbeB6_"
   },
   "source": [
    "# Under the Hood: pretrained models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Koytl9H4eL0W"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, from_pt=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmDP2ZS1eYAt"
   },
   "source": [
    "We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in words (or part of words, punctuation symbols, etc.) usually called tokens. There are multiple rules that can govern that process (you can learn more about them in the tokenizer summary), which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was pretrained.\n",
    "\n",
    "The second step is to convert those tokens into numbers, to be able to build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocab, which is the part we download when we instantiate it with the from_pretrained method, since we need to use the same vocab as when the model was pretrained.\n",
    "\n",
    "To apply these steps on a given text, we can just feed it to our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXnUhREdeotb"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ˜‰ Transformers library.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfigEZUJez8K"
   },
   "source": [
    "This returns a dictionary string to list of ints. It contains the ids of the tokens, as mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an attention mask that the model will use to have a better understanding of the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NADcJQLe0cD"
   },
   "outputs": [],
   "source": [
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJQ9M9Fze78S"
   },
   "source": [
    "You can pass a list of sentences directly to your tokenizer. If your goal is to send them through your model as a batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept and get tensors back. You can specify all of that to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-yHz2NAfAgy"
   },
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ˜‰ Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mxk5djSfFBB"
   },
   "source": [
    "The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding token the model was pretrained with. The attention mask is also adapted to take the padding into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87VjSa-NgLks"
   },
   "outputs": [],
   "source": [
    "for key, value in tf_batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LCEc6acgXRS"
   },
   "source": [
    "You can learn more about tokenizers here.\n",
    "\n",
    "Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will contain all the relevant information the model needs. If you're using a TensorFlow model, you can pass the dictionary keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2-dL3eRgXxK"
   },
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tf_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK8xTknvgjgA"
   },
   "source": [
    "In ðŸ˜‰ Transformers, all outputs are tuples (with only one element potentially). Here, we get a tuple with just the final activations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogmBgVPzgYJj"
   },
   "outputs": [],
   "source": [
    "print(tf_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCjl97fCgtff"
   },
   "source": [
    "The model can return more than just the final activations, which is why the output is a tuple. Here we only asked for the final activations, so we get a tuple with one element.\n",
    "\n",
    "**NOTE**: All ðŸ˜‰ Transformers models (PyTorch or TensorFlow) return the activations of the model before the final activation function (like SoftMax) since this final activation function is often fused with the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCYFUvlnhIw1"
   },
   "source": [
    "# Accessing the Code\n",
    "\n",
    "The AutoModel and AutoTokenizer classes are just shortcuts that will automatically work with any pretrained model. Behind the scenes, the library has one model class per combination of architecture plus class, so the code is easy to access and tweak if you need to.\n",
    "\n",
    "In our previous example, the model was called \"distilbert-base-uncased-finetuned-sst-2-english\", which means it's using the DistilBERT architecture. As AutoModelForSequenceClassification (or TFAutoModelForSequenceClassification if you are using TensorFlow) was used, the model automatically created is then a DistilBertForSequenceClassification. You can look at its documentation for all details relevant to that specific model, or browse the source code. This is how you would directly instantiate model and tokenizer without the auto magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpaueYEvhMXq"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfakBI9IhVTx"
   },
   "source": [
    "# Customize the Model\n",
    "\n",
    "If you want to change how the model itself is built, you can define your custom configuration class. Each architecture comes with its own relevant configuration (in the case of DistilBERT, DistilBertConfig) which allows you to specify any of the hidden dimension, dropout rate, etc. If you do core modifications, like changing the hidden size, you won't be able to use a pretrained model anymore and will need to train from scratch. You would then instantiate the model directly from this configuration.\n",
    "\n",
    "Here we use the predefined vocabulary of DistilBERT (hence load the tokenizer with the DistilBertTokenizer.from_pretrained method) and initialize the model from scratch (hence instantiate the model from the configuration instead of using the DistilBertForSequenceClassification.from_pretrained method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRHS1zLEhWd5"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLozrdY6ha4e"
   },
   "source": [
    "For something that only changes the head of the model (for instance, the number of labels), you can still use a pretrained model for the body. For instance, let's define a classifier for 10 different labels using a pretrained body. We could create a configuration with all the default values and just change the number of labels, but more easily, you can directly pass any argument a configuration would take to the from_pretrained method and it will update the default configuration with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aicR4vTOhe_O"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
